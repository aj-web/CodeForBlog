---
title: 特征工程基础
date: 2021年8月31日16:29:33
tags: 机器学习、特征工程
------

>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;在进行了数据收集，数据清洗，就要开始特征工程了，数据和特征决定了机器学习的上限，而模型和算法只是不断逼近这个上限而已，所以特征工程是非常重要的。这一节我们来学习特征工程。
<!--more-->

# 1.机器学习目标
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;在机器学习中，最为典型的分类算法和回归算法，他们的处理流程也是类似于这样一个过程。通过对历史数据的推断，寻找数字之间的规律，从而预测出后面的数字是什么样的。只不过，他要处理的问题，比这个简单的数字推断更复杂。机器学习的历史数据不再是一个一个的数字，而是由多个数字组成的向量。并且，数据之间的规律更难找到，同时也没有这么稳固。很可能数字之间并没有完全准确的规则，这时就需要选择出一个相对靠谱的数字来。其实这个思想跟之前的数字推断是一样的。所以总结来说机器学习处理的是数字向量之间的问题。

# 2.机器学习标准处理流程

- 数据收集：  
  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;机器学习会通过学习历史数据，总结出一些最有可能的规律。当这些规律达到一个比较高的可信度时，就可以用来对未来数据进行预测了。所以数据的体量以及质量，往往就决定了机器学习所能达到的高度。这也是为什么很多好的机器学习产品最先都是出在像谷歌、百度这样的大公司的，就是因为他们的数据往往是最大最全的。
  

- 2.数据清洗：  
  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;有了数据之后还需要进行清洗。原始的数据就像是矿石，往往含金量非常低。这个时候就要通过数据清洗，将明显无用的信息去掉，并且把数据整理成能够被机器学习接收的数据格式。这个过程通常没有固定的工作方式，需要根据不同的算法不同的要求，指定不同的处理方式。数据清洗是前期工作量非常大的一个环节，同时也是非常考验程序员工程能力的环节
  

- 3.训练模型：  
  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;这个过程是最关键的，但是其实他也是比较简单的。有了数据之后，你只需要选择一个合适的机器学习算法，把数据交给他学习，自然就会形成一个数据模型。这个过程往往不需要人工进行干预。甚至很多时候，机器学习到底学习到了哪些规律，人也是很难弄明白的。在这个过程中，需要注意的是，针对同一个问题，往往可以选择很多的算法，甚至针对同一个算法，也会需要制定不同的超参数。这些组合都会计算出不同的数据模型。所有这些模型都是可选的结果
  

- 4.模型优化：  
  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;训练出了模型，并不能代表机器学习成功。有了众多的数据模型之后，就需要在这些模型中找出针对当前问题的最佳方案。这个优化过程即需要基于对算法的深入了解，同时也需要基于大量的尝试。也是非常考验算法工程师技术的地方。  
  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;最常用的检测方案是将整个数据集随机拆分成训练集和测试集。用机器学习算法在训练集上学习并形成数据模型，然后拿这个数据模型对测试集的数据进行预测，接下来拿预测出来的目标值与测试集上实际的目标值进行比对。目标值真实结果匹配度最高的模型就认为是最好的模型。我们经常说的人脸识别准确率达到多少多少，其实就是在测试集上的比对结果。  
  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;另外，从历史数据中学习形成的数据模型，最终还是要回归于对未来业务的指导，而未来业务又会形成新的数据集。这个时候模型优化一个很重要的过程就是需要让模型继续学习更多新的业务数据，及时优化。这样才能让模型的准确地更高。

# 3. 常用的特征工程方法

## 3.1 特征抽取:  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;机器学习只能学习数字类型的特征值，但是有些数据集他的原始数据不是数字类型，比如图像、文本、字符等。这时，就需要使用特征抽取将数据转化成适合机器学习的数字特征。

### 3.1.1 文本特征抽取：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;场景：现在如果我们要对文本的分类情况进行机器学习，这是一个典型的分类问题，但是这个特征值似乎跟我们之前提到的特征值不太一样。文本没有那些属性和数字啊。那应该怎么抽取特征值呢？

### 3.1.1 one-hot编码：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;对于字典类型的数据特征，例如 性别、城市、颜色这一类字典类型数据，通常在数据库中，会以一个数字编码标识，如 0:男,1:女 这样。但是，如果在机器学习中使用这样的数字编码，就会给学习过程造成误解。因为不同的字典值特性应该是完全“平等”，而如果是 0，1，2这样的数字，则可能给机器学习造成误解，觉得这些字典值是有大小关系的。所以，机器学习中常用的方式是把字典值处理成one-hot编码。
![one-hot编码](https://raw.githubusercontent.com/aj-web/picturebed/master/one-hot%E7%BC%96%E7%A0%81.png)

### 3.1.2 CountVectorize：  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;对于文本类型的数据，如一篇文章。在做机器学习时，最基础的处理方式是以文章中的单词出现次数作为特征。处理成 [(word1,count1),(word2,count2)…]这样的格式。这也是mapreduce、spark最经典的入门计算方式。
```
缺点：在文章分类等机器学习场景中，体现不出文章的重要特征。例如一般出现次数最多的一些词，如，这里、那里、我们、他们等，并不能体现文章的内容特征。这类词称为停用词。
```

### 3.1.3 TfidfVectorizer：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;我们的目的是为了对文本进行分类，但是简单的以单词出现的次数来分类，从经验上判断，会有些问题。长的文章中各个词出现的次数都会比较多，而短的文章各个词普遍都会比较少。这样长的文章对分类结果的影响就会被放大。这时改进的办法就是TF-IDF  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;TF-IDF可以用来评估一个字词对于一个文件集合或者一个语料库中的其中一份文件的重要程度。例如，在对一大堆文章进行分类时，出现 计算机、软件、云、java这些词的次数比较多的文章更多可能归为科技类(在其他类中出现就比较少，这样的词才有重要性)，而出现 银行、信贷、信用卡 这类词出现次数较多的文章更多可能归为金融类。而所有文章中出现次数都比较多的 我们、你们、这里、那里等这一类的词则对分类来说，意义不大。

TF-IDF由TF和IDF两部分组成:  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;TF：词频 term frequency。某一个给定的词语在文章中出现的评率  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;IDF：逆向文档评率 inverse document frequency.是一个词语普遍重要性的度量。 为总文件数目 除以 包含该词语的文件的数量，再取 10为底对数。  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;最终TF-IDF=TF*IDF

示例： 关键词：“经济”；语料库： 1000篇文章；10篇文章出现“经济”。

TF(经济) = 10/1000 = 0.01 ；IDF(经济)=lg(1000/10)=2

最终 TF-IDF(经济) = TF(经济)*IDF(经济) = 0.02

## 3.2 特征预处理

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;现在我们考虑这样一组特征值： 用户年龄和用户收入。我们能发现，年龄的数字相比收入的数字会小很多。根据之前特征要平等的原则，直觉上就会觉得，这一组数据集中，用户收入的特征会被放大，而用户年龄的特征就容易被忽略。
![归一化](https://raw.githubusercontent.com/aj-web/picturebed/master/%E7%89%B9%E5%BE%81%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96.png)

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;你可能会想，这组数据，我把用户收入除100，是不是也就把收入固定到了0~100的范围，跟年龄差不多？这确实也是一种处理办法，但是，这样的处理方法，一方面，量纲的影响还是没有完全统一，范围并没有填满。另一方面，用户收入这个特征的很多信息其实就丢失了，拿去机器学习就会丢失很多特征，效果就不会太好。那业界比较常用的方式有两种，归一化 和 标准化。

### 3.2.1 归一化
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;归一化通过对原始数据进行变换把数据映射到[0,1]这样一个标准区间。而这个标准区间是可以根据实际情况调整的。
他的标准计算公式如下：
![归一化方程](https://raw.githubusercontent.com/aj-web/picturebed/master/%E5%BD%92%E4%B8%80%E5%8C%96%E6%96%B9%E7%A8%8B.png)

    其中max,min分别表示这一列特征值中的最大值和最小值。 而mx,mi表示指定的映射区间。默认mx为1，mi为0。
    归一化的缺点：对异常值敏感。当数据集中出现一个不太合理的极大值或者极小值时，整个归一化的结果就非常不好。鲁棒性(稳定性)较差

### 3.2.2 标准化
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;上面提到，归一化对异常值是非常敏感的。而标准化就能很好的处理这种异常数据的问题。  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;标准化是通过对原始数据进行变换，把数据变换到均值为0，标准差为1的范围内。  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;他的计算公式是：x'= (x-mean)/std  
mean： 特征值的均值， std：标准差。 均方差。  
这种方式，对于极大或极小的少量异常点，均值和标准差都会比较稳定，所以异常值的影响就变小了。

## 3.3 降维
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;降维是指在某些限定条件下，降低特征的个数，得到一组"不相关"的主变量的过程。那什么叫"不相关"呢？我们先简单的理解下什么叫特征与特征相关。例如，我们需要去学习某一个地区的降雨量，就会去统计一些常用的天气特征。而这其中，相对湿度与降雨量就是一个相关的特征，相对湿度大，肯定降雨量就会偏大。在进行机器学习训练算法时，如果特征本身存在问题或者特征之间相关性较强，那对于算法学习预测的影响就会比较大。而我们将为的过程，不光是要降低特征值的个数，同时也要尽量去除不相关的特征。

### 3.3.1 主特征选择
- 方差选择法： 过滤低方差特征(过于集中的数据)  
- 相关系数法：特征与特征之间的相关程度。例如 天气湿度 与 降雨量 一般就认为是相关性很强的特征。

### 3.3.2 主成分分析 
- 一种将高维数据转换为低维数据的方法。保留对目标值结果影响较大的特征值，去掉相对影响较小的特征值，例如拍照，三维空间中的人存储在二维的照片中
- 简单的理解就是特征个数太多，不好分析时，就可以用PCA来减少特征个数。